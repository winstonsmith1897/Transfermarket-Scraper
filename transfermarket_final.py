# -*- coding: utf-8 -*-
"""Transfermarket_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bMeD_2vXmXEcc4c2nZUsLsWaLwBuZTeZ
"""

import requests
import lxml
from bs4 import BeautifulSoup
import re


def player_crawler(primitive_url, link):
  url = link
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'
  }
  f = requests.get(url, headers = headers)
  soup = BeautifulSoup(f.content, 'lxml')
  pages = soup.find_all('td', class_= 'hauptlink')
  mid_url = []
  for i in range(0, len(pages)):
    if i % 2 == 0:
      players = str(pages[i]).split('a href="', 1)[1].split('">')[0]
      mid_url.append(primitive_url + players.split('" title=')[0])
  return mid_url

COMPETITIONS = "https://www.transfermarkt.co.uk/wettbewerbe/europa"
EXAMPLE_LINK = "https://www.transfermarkt.co.uk/juventus-turin/startseite/verein/506/saison_id/2021"
PRIMITIVE_URL = 'https://www.transfermarkt.co.uk'

def crawling_pages(primitive_url, source):
  url = source
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'
  }
  f = requests.get(url, headers = headers)
  soup = BeautifulSoup(f.content, 'lxml')
  comp = soup.find_all('td', class_= 'hauptlink')
  comp_url = []
  for i in tqdm(range(0, len(comp))):
    if i % 2 != 0:
        primo = str(comp[i]).split('" title="', 1)[1].split('" title="')[0]
        secondo = str(primo).split('<a href="',1)[1]
        comp_url.append(primitive_url + secondo)
  return comp_url

from tqdm import tqdm
competitions = crawling_pages(PRIMITIVE_URL, COMPETITIONS)

print(competitions[1])

def club_crawler(primitive_url, link):
  url = link
  headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36 QIHU 360SE'
  }
  f = requests.get(url, headers = headers)
  soup = BeautifulSoup(f.content, 'lxml')
  pages = soup.find_all('td', class_= 'hauptlink')
  mid_url = []
  try:
    for i in range(0, len(pages)):
      clubs = str(pages[i]).split('<td class="hauptlink no-border-links"><a href="', 1)[1].split('" title="')[0]
      mid_url.append(primitive_url + clubs)
  except IndexError:
    pass

  return mid_url

club_crawler(PRIMITIVE_URL, competitions[10])

profiles = player_crawler(PRIMITIVE_URL, EXAMPLE_LINK)

profiles

def connection(link):
  headers = {'User-Agent': 
           'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}
  # iterate from page 1 to page 10
  page = link 
  pageTree = requests.get(page, headers=headers)
  pageSoup = BeautifulSoup(pageTree.content, 'html.parser')
  #print(pageSoup)
  return pageSoup

import numpy as np
import random

#profile_url
def player_name(page):
  #page = connection(profile_url)
  player_name = page.find_all('title')
  name = str(player_name).split('<title>',1)[1].split(' - ')[0]
  #print(name)
  return name

def player_nationality(page):
  #page = connection(profile_url)
  player_nationality = page.find_all('li' ,class_="data-header__label")
  country = str(player_nationality).split('<img alt="', 1)[1].split('" class="flaggenrahmen"')[0]
  return country

def player_position(page):
  #page = connection(profile_url)
  auxiliary = page.find_all('li' ,class_="data-header__label")
  player_position = str(auxiliary).split('<span class="data-header__content">', 1)[1].split('</span>')[0]
  position = re.sub(' +', '', player_position.replace('\n', ''))
  #print(position)
  return position

def player_age(page):
  #page = connection(profile_url)
  auxiliary = page.find_all('li' ,class_="data-header__label")
  player_age = str(auxiliary).split('"birthDate">', 1)[1].split(' (')[0]
  age = re.sub(' +', ' ', player_age.replace('\n', ''))
  return age


def player_stats_auxiliary(page):
  #page = connection(profile_url)
  stats = page.find_all('td', class_="zentriert")
  stats = str(stats).split('</td>, <td class="zentriert"><a href="', 1)[1].split('</td>, <td class="no-border-rechts zentriert">')[0]
  return str(stats)

def appearances(page):
  stats = player_stats_auxiliary(page)
  apps = str(stats).split('" title="">', 1)[1].split('</a></td>')[0]
  return apps

def goals(page):
  stats = player_stats_auxiliary(page)
  apps = appearances(page)
  gol = str(stats).split(str(apps) + '</a></td>, <td class="zentriert">', 1)[1].split('</td>')[0]
  if gol == '-':
    gol = 0
  return gol

def assists(page):
  stats = player_stats_auxiliary(page)
  gol = goals(page)
  try:
    ass_ = str(stats).split('<td class="zentriert">' + str(gol) + '</td>, <td class="zentriert">', 1)[1].split('</td>,')[0]
    if ass_ == '-':
      ass_ = 0
  except IndexError:
    ass_ = 2
  return ass_

def minutes(page):
  sta = player_stats_auxiliary(page)
  ass_ = assists(page)
  if ass_ == 0:
    ass_ = '-'
  try:
    minutes_goal = str(sta).split('class="zentriert">' + str(ass_) + '</td>, <td class="zentriert">', 1)[1].split('</td>,')[0]
    if player_position(page) != 'Goalkeeper':
      minutes = str(sta).split('class="zentriert">' + str(minutes_goal) + '</td>, <td class="zentriert">', 1)[1].split('</td>')[0]
    else :
      minutes = minutes_goal
  except IndexError:
    minutes_goal = '-'
    minutes = '600'
  return minutes

def player_photo(page):
    #page = connection(profile_url)
    photos = page.find_all('div', class_="modal__content")
    #print(photos)
    photo_splitted = str(photos).split('" data-custom-close="" loading="lazy" src="',1)[1].split('" title="')[0]
    return photo_splitted

def player_team(page):
  #page = connection(profile_url)
  team = page.find_all('a', class_ = "data-header__box__club-link")
  team_splitted = str(team).split('<img alt="', 1)[1].split('" height=')[0]
  return team_splitted



def player_shirt(page):
  #page = connection(profile_url)
  shirt = page.find_all('span', class_ = "data-header__shirt-number")
  shirt_splitted = str(shirt).split('="data-header__shirt-number">', 1)[1].split('</span>')[0]
  shirt_number = re.sub(' +', '', shirt_splitted.replace('\n', '').replace('#', ''))
  return shirt_number

def yellow_card(page):
  yellows = int(random.uniform(0, 5))
  apps = int(appearances(page))
  if apps < yellows and apps > 1:
    yellows = apps - 1
  elif apps == 0:
    yellows = apps
  return yellows

def red_card(page):
  red = int(random.uniform(0, 2))
  apps = int(appearances(page))
  if apps < red and apps > 1:
    red = apps - 1
  elif apps == 0:
    red = apps
  return red


def value(page):
  #page = connection()
  value = page.find_all('meta')
  value_splitted = str(value).split('➤ Market value: ', 1)[1].split(' ➤')[0]
  return value_splitted

def player_rating(page):
  val = value(page)
  age = player_age(page)
  try:
    try:
      abs_val = int(val.split('£', 1)[1].split('.')[0])
    except ValueError:
      abs_val = int(val.split('£', 1)[1].split('Th.')[0]) * 0.1
    abs_age = 2022 - int(age.split(', ', 1)[1].split(' ')[0])
    mid_rating = (abs_age / abs_val)
    if abs_val >= abs_age :
      rating = mid_rating * 10
    else :
      rating = mid_rating
    if rating >= 8 or rating <= 4:
      rating = 6
  except IndexError:
    rating = 6
  return rating

from tqdm import tqdm

def create_player_card(id, profile):
  if player_position(profile) == 'Goalkeeper':
    Goalkeeper = {'_id':id, 'name':player_name(profile), 'nationality':player_nationality(profile), 'position':player_position(profile), 'age':player_age(profile), 
            'appearances':appearances(profile), 'conceded':goals(profile), 'clean-sheets':assists(profile), 'minutes':minutes(profile),
            'imageUrl':player_photo(profile), 'shirt_number':player_shirt(profile),'yellow':yellow_card(profile), 'red_card':red_card(profile), 'rating':player_rating(profile), 
            'value':value(profile), 'team':player_team(profile)}
    player = Goalkeeper
  else :
    player = {'_id':id, 'name':player_name(profile), 'team': player_team(profile),'nationality':player_nationality(profile), 'position':player_position(profile), 'age':player_age(profile), 
              'appearances':appearances(profile), 'goals':goals(profile), 'assists':assists(profile), 'minutes':minutes(profile),
              'imageUrl':player_photo(profile), 'shirt_number':player_shirt(profile),'yellow':yellow_card(profile), 'red_card':red_card(profile), 'rating':player_rating(profile),
              'value':value(profile), 'team':player_team(profile)}
  return player

players = player_crawler(PRIMITIVE_URL, 'https://www.transfermarkt.co.uk/manchester-city/startseite/verein/281/saison_id/2021')

print(players)

import json
def create_player_list():
  PLAYERS_LIST = []
  try :
    count = 1
    for competition in tqdm(competitions):
      print('***************************************************************')
      print(competition)
      clubs = club_crawler(PRIMITIVE_URL, competition)
      for club in tqdm(clubs):
        print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')
        print(club)
        players = player_crawler(PRIMITIVE_URL, club)
        try:
          for profile in players:
            page = connection(profile)
            player_card = create_player_card(count, page)
            print(player_card)
            PLAYERS_LIST.append(player_card)
            count = count + 1
        except IndexError:
          pass
  except IndexError:
    print('[-]ERROR DETECTED')
  
  return PLAYERS_LIST

try:
  players_list = create_player_list()
except KeyboardInterrupt: 
  print('[+] Interrupted')

from pymongo import MongoClient
  
try:
    conn = MongoClient('172.16.4.224', 27017)
    print("Connected successfully!!!")
except:  
    print("Could not connect to MongoDB")
  
# database
db = conn.database

collection = db.collection_prova
  
prova_player = {'_id': 1, 'name': 'Ederson', 'nationality': 'Brazil', 'position': 'Goalkeeper', 
                'age': ' Aug 17, 1993 ', 'appearances': '29', 'conceded': '18', 'clean-sheets': '17',
                'minutes': '2.610', 'imageUrl': 'https://img.a.transfermarkt.technology/portrait/big/238223-1474545573.jpg?lm=1',
                'shirt_number': '31', 'yellow': 3, 'red_card': 0, 'rating': 6.444444444444445, 'value': '£45.00m', 'team': 'Manchester City'}
  
# Insert Data
insertion = collection.insert_one(prova_player)
  
print("Data inserted with record ids",insertion)

'''
# Printing the data inserted
cursor = collection.find()
for record in cursor:
    print(record)
'''

from datetime import date

today = date.today()
print("Today's date:", today)

print(today)

stringa = ' Aug 17, 1993 '

int(stringa.split(', ', 1)[1].split(' ')[0])

print(2022-1995)

25 / 5

st = ' £45.00m'
st.split('£', 1)[1].split('.')[0]

